{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87c6dd18-c9fe-4802-81e8-f3f7cc91a73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "import time \n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16921f79-3c70-425a-9f1c-bea80a085642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "✓ Loaded 1000 videos\n",
      "✓ Video shape: (1000, 50, 32, 32)\n",
      "✓ Each video has 50 frames\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "data = np.load('../data/synthetic_tracking_dataset.npz', allow_pickle=True)\n",
    "videos = data['videos']\n",
    "positions = data['positions']\n",
    "\n",
    "print(f\"✓ Loaded {len(videos)} videos\")\n",
    "print(f\"✓ Video shape: {videos.shape}\")\n",
    "print(f\"✓ Each video has {videos.shape[1]} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d20009a7-b930-497c-a8c1-91b63e32c665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n",
      "✓ Training samples: 39200\n",
      "✓ Validation samples: 9800\n",
      "n✓ Batch input shape: torch.Size([32, 2, 32, 32])\n",
      "✓ Batch target shape: torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "class TrackingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for tracking task.\n",
    "    Given two consecutive frames, predict the position in the second frame.\n",
    "    \"\"\"\n",
    "    def __init__(self, videos, positions):\n",
    "        self.videos = videos\n",
    "        self.positions = positions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos) * (self.videos.shape[1] - 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        video_idx = idx // (self.videos.shape[1] - 1)\n",
    "        frame_idx = idx % (self.videos.shape[1] - 1)\n",
    "\n",
    "        frame1 = self.videos[video_idx, frame_idx]\n",
    "        frame2 = self.videos[video_idx, frame_idx + 1]\n",
    "\n",
    "        input_frames = np.stack([frame1, frame2], axis=0)\n",
    "\n",
    "        target_pos = np.array(self.positions[video_idx][frame_idx + 1])\n",
    "\n",
    "        input_frames = torch.FloatTensor(input_frames)\n",
    "        target_pos = torch.FloatTensor(target_pos)\n",
    "\n",
    "        return input_frames, target_pos\n",
    "\n",
    "print(\"Creating dataset...\")\n",
    "full_dataset = TrackingDataset(videos, positions)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "print(f\"✓ Training samples: {len(train_dataset)}\")\n",
    "print(f\"✓ Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_input, test_target = next(iter(train_loader))\n",
    "print(f\"n✓ Batch input shape: {test_input.shape}\")\n",
    "print(f\"✓ Batch target shape: {test_target.shape}\")\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b89ac8d1-0339-4ce7-a748-3a4df2cb8aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model components defined!\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Convert image into patches and embed them\"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=2, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.proj = nn.Conv2d( #convulational layer that extractes and embeds patches\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1,2)\n",
    "        return x\n",
    "\n",
    "class StandardAttention(nn.Module):\n",
    "    \"\"\"Standard multi-head self-attention\"\"\"\n",
    "    def __init__(self, dim, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape #unpack the input shape\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim) #creates qkv\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) #organize for multi-head attention\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale #compare patches - where you get the attentions scores\n",
    "        attn = attn.softmax(dim=-1) #convert to probabilities\n",
    "        attn = self.dropout(attn) #prevents overfitting\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C) #apply attention weights to values\n",
    "        x = self.proj(x) # refine output \n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"One transformer block: Attention + MLP\"\"\"\n",
    "    def __init__(self, dim, num_heads=4, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = StandardAttention(dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(), #activation function\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "print(\"✓ Model components defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5514533-2d9c-4277-9bb7-431e31176e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "✓ Model created with 208,418 parameters\n",
      "✓ Test output shape: torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "class VisionTransformerTracker(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer for tracking.\n",
    "    Takes two frames, outputs predicted (x, y) position.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=32,\n",
    "        patch_size=4,\n",
    "        in_channels=2,\n",
    "        embed_dim=64,\n",
    "        depth=4, #transformer blocks\n",
    "        num_heads=4, \n",
    "        mlp_ratio=4,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size, patch_size, in_channels, embed_dim\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim)) #tensors become paramters\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim) # stabilises numbers for final prediction head\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2), # compresses dims, 64 to 32\n",
    "            nn.GELU(), #adds non-linearity\n",
    "            nn.Linear(embed_dim // 2, 2) #returns two coordinates for prediction from 32 dims\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        x = x + self.pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        pos = self.head(x)\n",
    "\n",
    "        return pos\n",
    "\n",
    "print(\"Creating model...\")\n",
    "model = VisionTransformerTracker(\n",
    "    img_size=32,\n",
    "    patch_size=4,\n",
    "    embed_dim=64,\n",
    "    depth=4,\n",
    "    num_heads=4\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"✓ Model created with {total_params:,} parameters\")\n",
    "\n",
    "test_input = torch.randn(4, 2, 32, 32).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"✓ Test output shape: {test_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb9aa0-14b1-4b80-8488-e1854ee4dd53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
