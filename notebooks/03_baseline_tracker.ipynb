{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c6dd18-c9fe-4802-81e8-f3f7cc91a73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usinf device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "import time \n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16921f79-3c70-425a-9f1c-bea80a085642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "✓ Loaded 1000 videos\n",
      "✓ Video shape: (1000, 50, 32, 32)\n",
      "✓ Each video has 50 frames\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "data = np.load('../data/synthetic_tracking_dataset.npz', allow_pickle=True)\n",
    "videos = data['videos']\n",
    "positions = data['positions']\n",
    "\n",
    "print(f\"✓ Loaded {len(videos)} videos\")\n",
    "print(f\"✓ Video shape: {videos.shape}\")\n",
    "print(f\"✓ Each video has {videos.shape[1]} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d20009a7-b930-497c-a8c1-91b63e32c665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n",
      "✓ Training samples: 39200\n",
      "✓ Validation samples: 9800\n",
      "n✓ Batch input shape: torch.Size([32, 2, 32, 32])\n",
      "✓ Batch target shape: torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "class TrackingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for tracking task.\n",
    "    Given two consecutive frames, predict the position in the second frame.\n",
    "    \"\"\"\n",
    "    def __init__(self, videos, positions):\n",
    "        self.videos = videos\n",
    "        self.positions = positions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos) * (self.videos.shape[1] - 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        video_idx = idx // (self.videos.shape[1] - 1)\n",
    "        frame_idx = idx % (self.videos.shape[1] - 1)\n",
    "\n",
    "        frame1 = self.videos[video_idx, frame_idx]\n",
    "        frame2 = self.videos[video_idx, frame_idx + 1]\n",
    "\n",
    "        input_frames = np.stack([frame1, frame2], axis=0)\n",
    "\n",
    "        target_pos = np.array(self.positions[video_idx][frame_idx + 1])\n",
    "\n",
    "        input_frames = torch.FloatTensor(input_frames)\n",
    "        target_pos = torch.FloatTensor(target_pos)\n",
    "\n",
    "        return input_frames, target_pos\n",
    "\n",
    "print(\"Creating dataset...\")\n",
    "full_dataset = TrackingDataset(videos, positions)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "print(f\"✓ Training samples: {len(train_dataset)}\")\n",
    "print(f\"✓ Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_input, test_target = next(iter(train_loader))\n",
    "print(f\"n✓ Batch input shape: {test_input.shape}\")\n",
    "print(f\"✓ Batch target shape: {test_target.shape}\")\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ac8d1-0339-4ce7-a748-3a4df2cb8aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathEmbedding(nn.module):\n",
    "    \"\"\"Convert image into patches and embed them\"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=2, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.proj = nn.Conv2d( #convulational layer that extractes and embeds patches\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1,2)\n",
    "        return x\n",
    "\n",
    "class StandardAttention(nn.Module):\n",
    "    \"\"\"Standard multi-head self-attention\"\"\"\n",
    "    def __init__(self, dim, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv == nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape #unpack the input shape\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim) #creates qkv\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) #organize for multi-head attention\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, 1)) * self.scale #compare patches - where you get the attentions scores\n",
    "        attn = attn.softmax(dim=-1) #convert to probabilities\n",
    "        attn = self.dropout(attn) #prevents overfitting\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C) #apply attention weights to values\n",
    "        x = self.proj(x) # refine output \n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"One transformer block: Attention + MLP\"\"\"\n",
    "    def __init__(self, dim, num_heads=4, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = StandardAttention(dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        mlp_hdden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU #activation function\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "print(\"✓ Model components defined!\")\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
