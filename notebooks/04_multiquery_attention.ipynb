{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe82cb3-297b-41bd-ac80-c7f444778676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87de7234-b5b8-42f6-927f-6be0b6d0474e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Multi-query Attention...\n",
      "✔️ Input shape: torch.Size([2, 16, 64])\n",
      "✔️ Output shape: torch.Size([2, 16, 64])\n",
      "✔️ Multi - Query Attention parameters: 10,400\n"
     ]
    }
   ],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-query attention: multiple queries, but shared key and value.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5 #scales down attention scores so gradient remains stables \n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim) \n",
    "\n",
    "        self.k_proj = nn.Linear(dim, self.head_dim)\n",
    "        self.v_proj = nn.Linear(dim, self.head_dim)\n",
    "\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        q = self.q_proj(x).reshape(B, N, self.num_heads, self.head_dim)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "\n",
    "        k = self.k_proj(x) #shared K & V for all heads\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        k = k.unsqueeze(1)\n",
    "        v = v.unsqueeze(1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x = (attn @ v)\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "print(\"Testing Multi-query Attention...\")\n",
    "mqa = MultiQueryAttention(dim=64, num_heads=4)\n",
    "test_input = torch.randn(2, 16, 64)\n",
    "test_output = mqa(test_input)\n",
    "\n",
    "print(f\"✔️ Input shape: {test_input.shape}\")\n",
    "print(f\"✔️ Output shape: {test_output.shape}\")\n",
    "\n",
    "mqa_params = sum(p.numel() for p in mqa.parameters()) #counts every weight in the model\n",
    "print(f\"✔️ Multi - Query Attention parameters: {mqa_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b620233e-9862-423d-a530-413d6e19d6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PARAMETER COMPARISON\n",
      "============================================================\n",
      "Standard Attention: 16,640 parameters\n",
      "Multi-Query Attention: 10,400 paramters\n",
      "Reduction:              6,240 parameters (37.5% fewer)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "class StandardAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "std_attn = StandardAttention(dim=64, num_heads=4)\n",
    "mq_attn = MultiQueryAttention(dim=64, num_heads=4)\n",
    "\n",
    "std_params = sum(p.numel() for p in std_attn.parameters())\n",
    "mq_params = sum(p.numel() for p in mq_attn.parameters())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARAMETER COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Standard Attention: {std_params:,} parameters\")\n",
    "print(f\"Multi-Query Attention: {mq_params:,} paramters\")\n",
    "print(f\"Reduction:              {std_params - mq_params:,} parameters ({(1 - mq_params/std_params)*100:.1f}% fewer)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ec2a8-fbca-4761-978a-4df2caf7ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy these from Day 3 (same as before)\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=2, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block - now using Multi-Query Attention!\"\"\"\n",
    "    def __init__(self, dim, num_heads=4, mlp_ratio=4, dropout=0.1, use_mqa=True):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        \n",
    "        # Choose attention type\n",
    "        if use_mqa:\n",
    "            self.attn = MultiQueryAttention(dim, num_heads, dropout)\n",
    "        else:\n",
    "            self.attn = StandardAttention(dim, num_heads, dropout)\n",
    "            \n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        \n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "print(\"✓ Building blocks defined!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
