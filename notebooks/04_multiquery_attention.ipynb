{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fe82cb3-297b-41bd-ac80-c7f444778676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87de7234-b5b8-42f6-927f-6be0b6d0474e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Multi-query Attention...\n",
      "✔️ Input shape: torch.Size([2, 16, 64])\n",
      "✔️ Output shape: torch.Size([2, 16, 64])\n",
      "✔️ Multi - Query Attention parameters: 10,400\n"
     ]
    }
   ],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-query attention: multiple queries, but shared key and value.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5 #scales down attention scores so gradient remains stables \n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim) \n",
    "\n",
    "        self.k_proj = nn.Linear(dim, self.head_dim)\n",
    "        self.v_proj = nn.Linear(dim, self.head_dim)\n",
    "\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        q = self.q_proj(x).reshape(B, N, self.num_heads, self.head_dim)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "\n",
    "        k = self.k_proj(x) #shared K & V for all heads\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        k = k.unsqueeze(1)\n",
    "        v = v.unsqueeze(1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x = (attn @ v)\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "print(\"Testing Multi-query Attention...\")\n",
    "mqa = MultiQueryAttention(dim=64, num_heads=4)\n",
    "test_input = torch.randn(2, 16, 64)\n",
    "test_output = mqa(test_input)\n",
    "\n",
    "print(f\"✔️ Input shape: {test_input.shape}\")\n",
    "print(f\"✔️ Output shape: {test_output.shape}\")\n",
    "\n",
    "mqa_params = sum(p.numel() for p in mqa.parameters()) #counts every weight in the model\n",
    "print(f\"✔️ Multi - Query Attention parameters: {mqa_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b620233e-9862-423d-a530-413d6e19d6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PARAMETER COMPARISON\n",
      "============================================================\n",
      "Standard Attention: 16,640 parameters\n",
      "Multi-Query Attention: 10,400 paramters\n",
      "Reduction:              6,240 parameters (37.5% fewer)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "class StandardAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "std_attn = StandardAttention(dim=64, num_heads=4)\n",
    "mq_attn = MultiQueryAttention(dim=64, num_heads=4)\n",
    "\n",
    "std_params = sum(p.numel() for p in std_attn.parameters())\n",
    "mq_params = sum(p.numel() for p in mq_attn.parameters())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARAMETER COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Standard Attention: {std_params:,} parameters\")\n",
    "print(f\"Multi-Query Attention: {mq_params:,} paramters\")\n",
    "print(f\"Reduction:              {std_params - mq_params:,} parameters ({(1 - mq_params/std_params)*100:.1f}% fewer)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "587ec2a8-fbca-4761-978a-4df2caf7ab52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Building blocks defined!\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=2, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block - now using Multi-Query Attention!\"\"\"\n",
    "    def __init__(self, dim, num_heads=4, mlp_ratio=4, dropout=0.1, use_mqa=True):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        \n",
    "        # Choose attention type\n",
    "        if use_mqa:\n",
    "            self.attn = MultiQueryAttention(dim, num_heads, dropout)\n",
    "        else:\n",
    "            self.attn = StandardAttention(dim, num_heads, dropout)\n",
    "            \n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        \n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "print(\"✓ Building blocks defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d30eb609-ad56-4abe-bd9e-e2c29b6bd1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Multi-Query Attention model...\n",
      "✔️ Multi-Query model: 183,458 parameters\n",
      "✔️ Output shape: torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "class VisionTransformerTracker(nn.Module):\n",
    "    \"\"\" Vision transformer for tracking - with Multi-Query Attention \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=32,\n",
    "        patch_size=4,\n",
    "        in_channels=2,\n",
    "        embed_dim=64,\n",
    "        depth=4,\n",
    "        num_heads=4,\n",
    "        mlp_ratio=4,\n",
    "        dropout=0.1,\n",
    "        use_mqa=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #Transformer blocks with Multi-Query attention\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout, use_mqa=use_mqa)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim // 2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=1)\n",
    "        pos = self.head(x)\n",
    "\n",
    "        return pos\n",
    "\n",
    "print(\"Creating Multi-Query Attention model...\")\n",
    "mq_model = VisionTransformerTracker(\n",
    "    img_size=32,\n",
    "    patch_size=4,\n",
    "    embed_dim=64,\n",
    "    depth=4,\n",
    "    num_heads=4,\n",
    "    use_mqa=True\n",
    ").to(device)\n",
    "\n",
    "mq_params = sum(p.numel() for p in mq_model.parameters())\n",
    "print(f\"✔️ Multi-Query model: {mq_params:,} parameters\")\n",
    "\n",
    "test_input = torch.randn(4, 2, 32, 32).to(device)\n",
    "test_output = mq_model(test_input)\n",
    "print(f\"✔️ Output shape: {test_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b6fe0db-2b70-4558-bfe1-63d03214121e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "✔️ Training samples: 39200\n",
      "✔️ Validation samples: 9800\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "data = np.load('../data/synthetic_tracking_dataset.npz', allow_pickle=True)\n",
    "videos = data['videos']\n",
    "positions = data['positions']\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TrackingDataset(Dataset):\n",
    "    def __init__(self, videos, positions):\n",
    "        self.videos = videos\n",
    "        self.positions = positions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos) * (self.videos.shape[1] - 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_idx = idx // (self.videos.shape[1] - 1)\n",
    "        frame_idx = idx % (self.videos.shape[1] - 1)\n",
    "\n",
    "        frame1 = self.videos[video_idx, frame_idx]\n",
    "        frame2 = self.videos[video_idx, frame_idx + 1]\n",
    "        input_frames = np.stack([frame1, frame2], axis=0)\n",
    "        target_pos = np.array(self.positions[video_idx][frame_idx + 1])\n",
    "\n",
    "        return torch.FloatTensor(input_frames), torch.FloatTensor(target_pos)\n",
    "\n",
    "full_dataset = TrackingDataset(videos, positions)\n",
    "train_size = int(0.8 *  len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"✔️ Training samples: {len(train_dataset)}\")\n",
    "print(f\"✔️ Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cfc44c7-1877-4359-8992-737bae79f4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Training functions ready!\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, targets in tqdm(dataLoader, desc=\"Training\"):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "print(\"✔️ Training functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790e2e5-2622-46d2-81f6-6a17cc7ef427",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(mq_model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Training Multi-Query Attention model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(mq_model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    val_loss = validate(mq_model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\" Train Loss: {train_loss:.4f}\")\n",
    "    print(f\" Val Loss: {val_loss:.4f}\")\n",
    "    print(\"-\"*60)\n",
    "print(\"\\n✔️ Training completed!\")\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Multi-Query Attention Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
